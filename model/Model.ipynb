{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIaBtgQ2GCNL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1MbHQuQGKhJ"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# IMAGE_DIR = \"/content/drive/MyDrive/Images_RGB\"\n",
        "# MASK_DIR = \"/content/drive/MyDrive/masks\"\n",
        "\n",
        "IMAGE_DIR = \"/content/images\"\n",
        "MASK_DIR = \"/content/masks\"\n",
        "\n",
        "\n",
        "# Configuration\n",
        "IMAGE_SIZE = (256, 256)\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 50\n",
        "NUM_CLASSES = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91zeiBPZGYdM",
        "outputId": "c14e25ff-891a-4d0c-cc48-6e9850b0dcdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images shape: (100, 256, 256, 3)\n",
            "Masks shape: (100, 256, 256, 1)\n"
          ]
        }
      ],
      "source": [
        "def load_and_preprocess(folder, target_size, grayscale=False):\n",
        "    images = []\n",
        "    filenames = sorted(os.listdir(folder))\n",
        "    for filename in filenames:\n",
        "        path = os.path.join(folder, filename)\n",
        "        # Read as grayscale or color based on flag\n",
        "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE if grayscale else cv2.IMREAD_COLOR)\n",
        "        if img is not None:\n",
        "            # Use INTER_NEAREST for masks, INTER_AREA for images\n",
        "            # Use interpolation for segmentation\n",
        "            interp = cv2.INTER_NEAREST if grayscale else cv2.INTER_AREA\n",
        "            img = cv2.resize(img, target_size, interpolation=interp)\n",
        "            if grayscale:\n",
        "                # Expand dims so that mask shape becomes (H, W, 1) and threshold it\n",
        "                img = np.expand_dims(img, axis=-1)\n",
        "                img = (img > 127).astype(np.float32) #Threshold for masks\n",
        "            else:\n",
        "                img = img.astype(np.float32) / 255.0\n",
        "            images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Load images and masks\n",
        "images = load_and_preprocess(IMAGE_DIR, IMAGE_SIZE, grayscale=False)\n",
        "masks  = load_and_preprocess(MASK_DIR, IMAGE_SIZE, grayscale=True)\n",
        "print(\"Images shape:\", images.shape)\n",
        "print(\"Masks shape:\", masks.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CNyptPkGqiA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "    images, masks, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjW4orR5G_4M"
      },
      "outputs": [],
      "source": [
        "# Augmentation for Welding Images\n",
        "augmentor = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=15, p=0.4),\n",
        "    A.RandomBrightnessContrast(p=0.3),\n",
        "    A.CLAHE(p=0.3),\n",
        "    A.GaussianBlur(blur_limit=(3,5), p=0.1),\n",
        "    # Note: Do not apply normalization here since we want inputs in [0,1]\n",
        "])\n",
        "\n",
        "def _augment(image_np, mask_np):\n",
        "    # Ensure inputs are NumPy arrays (this fixes the EagerTensor issue)\n",
        "    image_np = np.array(image_np)\n",
        "    mask_np = np.array(mask_np)\n",
        "    # Albumentations expects uint8 images\n",
        "    image_np = (image_np * 255).astype(np.uint8)\n",
        "    mask_np = (mask_np * 255).astype(np.uint8)\n",
        "\n",
        "    augmented = augmentor(image=image_np, mask=mask_np)\n",
        "    aug_img = augmented['image'].astype(np.float32) / 255.0  # Scale back to [0,1]\n",
        "    aug_mask = augmented['mask'].astype(np.float32) / 255.0\n",
        "    if aug_mask.ndim == 2:\n",
        "        aug_mask = np.expand_dims(aug_mask, axis=-1)\n",
        "    return aug_img, aug_mask\n",
        "\n",
        "def augment_data(image, mask):\n",
        "    aug_img, aug_mask = tf.py_function(\n",
        "        func=_augment,\n",
        "        inp=[image, mask],\n",
        "        Tout=[tf.float32, tf.float32]\n",
        "    )\n",
        "    # Set static shapes for proper batching\n",
        "    aug_img.set_shape(IMAGE_SIZE + (3,))\n",
        "    aug_mask.set_shape(IMAGE_SIZE + (1,))\n",
        "    return aug_img, aug_mask\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfpGxX9vHRfK"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(images, masks, training=True):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((images, masks))\n",
        "    if training:\n",
        "      #Shuffling prevents the model from learning order-dependent patterns in the data(shuffles entire dataset)\n",
        "        ds = ds.shuffle(buffer_size=len(images))\n",
        "      #ds.map(augment_data) applies the augment_data function to each (image, mask) pair.\n",
        "      #AUTOTUNE automatically optimizes the number of parallel processes for faster execution.\n",
        "        ds = ds.map(augment_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    #Batches multiple samples together into groups of size BATCH_SIZE.\n",
        "    ds = ds.batch(BATCH_SIZE)\n",
        "    #Prefetching allows the model to load the next batch while training on the current batch.\n",
        "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_dataset = prepare_dataset(train_images, train_masks, training=True)\n",
        "val_dataset   = prepare_dataset(val_images, val_masks, training=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HNMgp3qHXHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ca4367-8eda-4e27-da8b-7ec211ca231a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_cv\n",
            "  Downloading keras_cv-0.9.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras_cv) (24.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras_cv) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from keras_cv) (2024.11.6)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (from keras_cv) (4.9.7)\n",
            "Collecting keras-core (from keras_cv)\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/95/f7/b8dcff937ea64f822f0d3fe8c6010793406b82d14467cd0e9eecea458a40/keras_core-0.1.7-py3-none-any.whl.metadata\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from keras_cv) (0.3.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras_cv) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras_cv) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras_cv) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (3.12.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (0.1.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (8.1.8)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (4.25.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (17.0.0)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (1.16.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (2.5.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (1.17.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (0.6.0)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (1.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (4.12.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_cv) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_cv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_cv) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_cv) (2025.1.31)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->keras-core->keras_cv) (25.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from promise->tensorflow-datasets->keras_cv) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core->keras_cv) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core->keras_cv) (2.18.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple-parsing->tensorflow-datasets->keras_cv) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras_cv) (1.67.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras_cv) (0.1.2)\n",
            "Downloading keras_cv-0.9.0-py3-none-any.whl (650 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-core, keras_cv\n",
            "Successfully installed keras-core-0.1.7 keras_cv-0.9.0\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/resnetv2/keras/resnet50_v2_imagenet/2/download/config.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 886/886 [00:00<00:00, 2.09MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/resnetv2/keras/resnet50_v2_imagenet/2/download/model.weights.h5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90.3M/90.3M [00:07<00:00, 12.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install keras_cv\n",
        "import keras_cv\n",
        "# Use ResNet50V2 as backbone (pretrained on ImageNet) as in Code 1\n",
        "backbone = keras_cv.models.ResNet50V2Backbone.from_preset(\n",
        "    preset=\"resnet50_v2_imagenet\",\n",
        "    input_shape=IMAGE_SIZE + (3,),\n",
        "    load_weights=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou206i7fHbDB"
      },
      "outputs": [],
      "source": [
        "model = keras_cv.models.segmentation.DeepLabV3Plus(\n",
        "    num_classes=NUM_CLASSES,\n",
        "    backbone=backbone\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9ivClN3RIcu"
      },
      "outputs": [],
      "source": [
        "def mean_iou(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute mean IoU over the batch. Expects y_true to be (batch, H, W, 1)\n",
        "    and y_pred to be logits or probabilities of shape (batch, H, W, num_classes).\n",
        "    \"\"\"\n",
        "    # Squeeze last channel of ground truth and one-hot encode\n",
        "    y_true = tf.squeeze(y_true, axis=-1)\n",
        "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), NUM_CLASSES)\n",
        "    # Use argmax to get predictions if necessary\n",
        "    if y_pred.shape[-1] > 1:\n",
        "        y_pred = tf.one_hot(tf.argmax(y_pred, axis=-1), NUM_CLASSES)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2])\n",
        "    union = tf.reduce_sum(y_true + y_pred, axis=[1,2]) - intersection\n",
        "    iou = tf.math.divide_no_nan(intersection, union)\n",
        "    return tf.reduce_mean(iou)\n",
        "\n",
        "# Compile the model with a standard loss and metrics\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmpZGNf-IejK"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=loss_fn,\n",
        "    metrics=[\"accuracy\", mean_iou]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ2gvO1XIlJB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "05f230c4-ef09-4835-cc76-6d8984442a07"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"deep_lab_v3_plus\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"deep_lab_v3_plus\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional (\u001b[38;5;33mFunctional\u001b[0m)   │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │     \u001b[38;5;34m23,556,608\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m2048\u001b[0m)]    │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ spatial_pyramid_pooling   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m15,538,176\u001b[0m │ functional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mSpatialPyramidPooling\u001b[0m)   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_output_upsampling │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ spatial_pyramid_pooli… │\n",
              "│ (\u001b[38;5;33mUpSampling2D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ sequential_6 (\u001b[38;5;33mSequential\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m48\u001b[0m)     │         \u001b[38;5;34m12,480\u001b[0m │ functional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m304\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ encoder_output_upsamp… │\n",
              "│                           │                        │                │ sequential_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ sequential_7 (\u001b[38;5;33mSequential\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m2\u001b[0m)    │         \u001b[38;5;34m79,360\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">23,556,608</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)]    │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ spatial_pyramid_pooling   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,538,176</span> │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialPyramidPooling</span>)   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_output_upsampling │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ spatial_pyramid_pooli… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ sequential_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12,480</span> │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">304</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_output_upsamp… │\n",
              "│                           │                        │                │ sequential_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ sequential_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">79,360</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m39,186,624\u001b[0m (149.49 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">39,186,624</span> (149.49 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m39,141,600\u001b[0m (149.31 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">39,141,600</span> (149.31 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m45,024\u001b[0m (175.88 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,024</span> (175.88 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"best_deeplabv3plus.h5\",\n",
        "    monitor=\"val_mean_iou\",\n",
        "    mode=\"max\",\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
        "\n",
        "callbacks = [checkpoint_cb, tensorboard_cb]\n",
        "\n"
      ],
      "metadata": {
        "id": "O6K2IRr3DzHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3A3x-HtD5Gv",
        "outputId": "7b63080a-b8b7-48be-ab81-89d22e6d8d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615ms/step - accuracy: 0.9913 - loss: 0.0379 - mean_iou: 0.8241\n",
            "Epoch 1: val_mean_iou did not improve from 0.49034\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 661ms/step - accuracy: 0.9913 - loss: 0.0376 - mean_iou: 0.8257 - val_accuracy: 0.9690 - val_loss: 0.1285 - val_mean_iou: 0.4849\n",
            "Epoch 2/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638ms/step - accuracy: 0.9930 - loss: 0.0324 - mean_iou: 0.8584\n",
            "Epoch 2: val_mean_iou did not improve from 0.49034\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 682ms/step - accuracy: 0.9930 - loss: 0.0324 - mean_iou: 0.8580 - val_accuracy: 0.9699 - val_loss: 0.1282 - val_mean_iou: 0.4854\n",
            "Epoch 3/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599ms/step - accuracy: 0.9935 - loss: 0.0296 - mean_iou: 0.8584\n",
            "Epoch 3: val_mean_iou did not improve from 0.49034\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 640ms/step - accuracy: 0.9935 - loss: 0.0297 - mean_iou: 0.8578 - val_accuracy: 0.9724 - val_loss: 0.1254 - val_mean_iou: 0.4866\n",
            "Epoch 4/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561ms/step - accuracy: 0.9935 - loss: 0.0289 - mean_iou: 0.8670\n",
            "Epoch 4: val_mean_iou did not improve from 0.49034\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 603ms/step - accuracy: 0.9935 - loss: 0.0289 - mean_iou: 0.8670 - val_accuracy: 0.9725 - val_loss: 0.1312 - val_mean_iou: 0.4866\n",
            "Epoch 5/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568ms/step - accuracy: 0.9934 - loss: 0.0280 - mean_iou: 0.8590\n",
            "Epoch 5: val_mean_iou did not improve from 0.49034\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 610ms/step - accuracy: 0.9933 - loss: 0.0280 - mean_iou: 0.8591 - val_accuracy: 0.9783 - val_loss: 0.1313 - val_mean_iou: 0.4896\n",
            "Epoch 6/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583ms/step - accuracy: 0.9934 - loss: 0.0269 - mean_iou: 0.8658\n",
            "Epoch 6: val_mean_iou did not improve from 0.49034\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 625ms/step - accuracy: 0.9935 - loss: 0.0268 - mean_iou: 0.8657 - val_accuracy: 0.9743 - val_loss: 0.1368 - val_mean_iou: 0.4876\n",
            "Epoch 7/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589ms/step - accuracy: 0.9935 - loss: 0.0269 - mean_iou: 0.8716\n",
            "Epoch 7: val_mean_iou did not improve from 0.49034\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 632ms/step - accuracy: 0.9935 - loss: 0.0269 - mean_iou: 0.8712 - val_accuracy: 0.9796 - val_loss: 0.1260 - val_mean_iou: 0.4902\n",
            "Epoch 8/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583ms/step - accuracy: 0.9930 - loss: 0.0263 - mean_iou: 0.8604\n",
            "Epoch 8: val_mean_iou did not improve from 0.49034\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 628ms/step - accuracy: 0.9931 - loss: 0.0262 - mean_iou: 0.8612 - val_accuracy: 0.9799 - val_loss: 0.1304 - val_mean_iou: 0.4903\n",
            "Epoch 9/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593ms/step - accuracy: 0.9943 - loss: 0.0238 - mean_iou: 0.8851\n",
            "Epoch 9: val_mean_iou did not improve from 0.49034\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 643ms/step - accuracy: 0.9942 - loss: 0.0239 - mean_iou: 0.8836 - val_accuracy: 0.9799 - val_loss: 0.1307 - val_mean_iou: 0.4903\n",
            "Epoch 10/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609ms/step - accuracy: 0.9939 - loss: 0.0243 - mean_iou: 0.8713\n",
            "Epoch 10: val_mean_iou did not improve from 0.49034\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 653ms/step - accuracy: 0.9939 - loss: 0.0244 - mean_iou: 0.8714 - val_accuracy: 0.9799 - val_loss: 0.1365 - val_mean_iou: 0.4903\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}